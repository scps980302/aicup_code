{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n",
    "!pip install adabelief-pytorch==0.2.0\n",
    "!pip install monai\n",
    "!pip install torchvision\n",
    "\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!pip install ipywidgets widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define shape_to_mask function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_mask(\n",
    "    img_shape, points, shape_type=None, line_width=10, point_size=5\n",
    "):\n",
    "    mask = np.zeros(img_shape[:2], dtype=np.uint8) \n",
    "    mask = PIL.Image.fromarray(mask) \n",
    "    \n",
    "    draw = PIL.ImageDraw.Draw(mask) \n",
    "\n",
    "    xy = [tuple(point) for point in points] \n",
    "    \n",
    "    \n",
    "    if shape_type == \"circle\":\n",
    "        assert len(xy) == 2, \"Shape of shape_type=circle must have 2 points\"\n",
    "        (cx, cy), (px, py) = xy\n",
    "        d = math.sqrt((cx - px) ** 2 + (cy - py) ** 2)\n",
    "        draw.ellipse([cx - d, cy - d, cx + d, cy + d], outline=1, fill=1) \n",
    "    elif shape_type == \"rectangle\":\n",
    "        assert len(xy) == 2, \"Shape of shape_type=rectangle must have 2 points\"\n",
    "        draw.rectangle(xy, outline=1, fill=1)\n",
    "    elif shape_type == \"line\":\n",
    "        assert len(xy) == 2, \"Shape of shape_type=line must have 2 points\"\n",
    "        draw.line(xy=xy, fill=1, width=line_width)\n",
    "    elif shape_type == \"linestrip\":\n",
    "        draw.line(xy=xy, fill=1, width=line_width)\n",
    "    elif shape_type == \"point\":\n",
    "        assert len(xy) == 1, \"Shape of shape_type=point must have 1 points\"\n",
    "        cx, cy = xy[0]\n",
    "        r = point_size\n",
    "        draw.ellipse([cx - r, cy - r, cx + r, cy + r], outline=1, fill=1)\n",
    "    else:\n",
    "        assert len(xy) > 2, \"Polygon must have points more than 2\" \n",
    "        draw.polygon(xy=xy, outline=1, fill=1)\n",
    "    mask = np.array(mask, dtype=bool)\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set process folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path\n",
    "folder_path = \"SEG_Train_Datasets/Train_Annotations/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Visualize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i, (name, image) in enumerate(images.items()):   \n",
    "        plt.subplot(1, n, i + 1)  \n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing all json file in the folder (remember to create a \"msk_img\" folder for storing the results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.makedirs(folder_path.replace(\"Train_Annotations\", \"Train_Annotations_png\"))\n",
    "except: \n",
    "    pass\n",
    "\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if 'json' in filename:\n",
    "        # Read in all all the data from the CSV file  \n",
    "        json_path = os.path.join(folder_path, filename)       \n",
    "        \n",
    "        write_msk_img_name = filename.replace(\"json\",\"png\")\n",
    "        write_folder_path = folder_path.replace(\"Train_Annotations\",\"Train_Annotations_png\")\n",
    "        \n",
    "        img_path = folder_path.replace(\"/Train_Annotations/\",\"/Train_Images/\") + filename.replace(\"json\",\"jpg\")\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        #Read Json file\n",
    "        with open(json_path, \"r\",encoding=\"utf-8\") as f:\n",
    "            dj = json.load(f)\n",
    "\n",
    "        \n",
    "        temp_mask_img = np.zeros([dj['imageHeight'], dj['imageWidth']],dtype=np.uint8)\n",
    "\n",
    "        #Plot each mask into mask_img\n",
    "        for i in range(len(dj['shapes'])):\n",
    "            mask = shape_to_mask((dj['imageHeight'],dj['imageWidth']), dj['shapes'][i]['points'], shape_type=dj['shapes'][i]['shape_type'],line_width=1, point_size=1)            \n",
    "            temp_mask_img = temp_mask_img + mask.astype(int) \n",
    "        temp_mask_img = (temp_mask_img>0).astype(int)\n",
    "        \n",
    "        if int(filename.split('.')[0])%300 == 0:\n",
    "            visualize(\n",
    "                image = img,\n",
    "                mask = temp_mask_img\n",
    "                )\n",
    "        print(f\"temp_mask_img.range is {temp_mask_img.max()} to {temp_mask_img.min()}\")\n",
    "        # Save the file\n",
    "        cv2.imwrite(write_folder_path + write_msk_img_name, temp_mask_img*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages\n",
    "\n",
    "1. [logging](https://docs.python.org/zh-tw/3/howto/logging.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import glob\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import setuptools\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import monai\n",
    "from monai.data import create_test_image_2d, list_data_collate, decollate_batch\n",
    "from monai.inferers import sliding_window_inference, SimpleInferer\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AddChanneld,\n",
    "    AsDiscrete,\n",
    "    Compose,  \n",
    "    LoadImaged,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandRotate90d,\n",
    "    ScaleIntensityd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    AsChannelFirstd,\n",
    "    AsChannelLast,\n",
    "    Resized,\n",
    "#     RandScaleCropd,\n",
    "    RandRotated,\n",
    "    SaveImage,\n",
    "#     RandSpatialCropd,\n",
    "    Resize,\n",
    "    RandFlipd,\n",
    "#     RandRotate,\n",
    ")\n",
    "from monai.visualize import plot_2d_or_3d_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check MONAI configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monai.config.print_config()\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process VGH Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Data folder\n",
    "data_path = './SEG_Train_Datasets/'\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train files\n",
    "for_test = 253\n",
    "\n",
    "\n",
    "tempdir = os.path.join(data_path, \"Train_Images\")\n",
    "train_images = sorted(glob.glob(os.path.join(tempdir, \"*.jpg\")))\n",
    "\n",
    "tempdir = os.path.join(data_path, \"Train_Annotations_png\")\n",
    "train_segs = sorted(glob.glob(os.path.join(tempdir, \"*.png\")))\n",
    "\n",
    "\n",
    "print(f\" {len(train_images[for_test:])} train_images and {len(train_segs[for_test:])} train_segs\")\n",
    "\n",
    "\n",
    "train_files = [{\"img\": img, \"seg\": seg} for img, seg in zip(train_images[for_test:], train_segs[for_test:])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data divided into Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_files = [{\"img\": img, \"seg\": seg} for img, seg in zip(train_images[:for_test], train_segs[:for_test])]\n",
    "print(f\" {len(train_images[:for_test])} val_images and {len(train_segs[:for_test])} val_segs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Trasform for image and segmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms for image and segmentation\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\", \"seg\"]),\n",
    "        AddChanneld(keys=[\"seg\"]),        \n",
    "        AsChannelFirstd(keys=[\"img\"]),\n",
    "        ScaleIntensityd(keys=[\"img\", \"seg\"]),\n",
    "        RandFlipd(keys=[\"img\", \"seg\"], prob=0.3),\n",
    "        RandRotated(keys=[\"img\", \"seg\"],range_x= 5),\n",
    "        Resized(keys=[\"img\", \"seg\"], spatial_size=[800, 800]),\n",
    "        EnsureTyped(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\", \"seg\"]),\n",
    "        AddChanneld(keys=[\"seg\"]),        \n",
    "        AsChannelFirstd(keys=[\"img\"]),\n",
    "        ScaleIntensityd(keys=[\"img\", \"seg\"]),\n",
    "        Resized(keys=[\"img\"], spatial_size=[800, 800]),\n",
    "        EnsureTyped(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataLoader for train and validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size \n",
    "batch_size = 16\n",
    "# create a training data loader\n",
    "train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size= batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=list_data_collate,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size= batch_size, collate_fn=list_data_collate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define metric and post-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "post_trans = Compose([EnsureType(), Activations(sigmoid=True), AsDiscrete(threshold=0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1, 2, 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Visualize Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image, cmap= 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open TensorBoard](/tensorboard/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_mod = 'EfficientV2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smp官方資訊\n",
    "aux_params=dict(\n",
    "    pooling='avg',             # one of 'avg', 'max'\n",
    "    dropout=0.4,               # dropout ratio, default is None\n",
    "    activation=None,           # activation function, default is None\n",
    "    classes=1,                 # define number of output labels\n",
    ")\n",
    "\n",
    "\n",
    "encoder_type = 'tu-tf_efficientnetv2_m_in21ft1k'\n",
    "model_no = smp.DeepLabV3Plus(encoder_type, aux_params=aux_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model_no).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBelief\n",
    "from adabelief_pytorch import AdaBelief\n",
    "optimizer = AdaBelief(model.parameters(), lr=1.5e-4, eps=1e-16, betas=(0.9, 0.99), weight_decouple = True, rectify = False, weight_decay = 1e-4)\n",
    "loss_function = monai.losses.DiceLoss(sigmoid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### start a typical PyTorch training\n",
    "total_epochs = 500\n",
    "val_interval = 1\n",
    "best_metric = 100  \n",
    "best_metric_epoch = -1 \n",
    "epoch_loss_values = list()  \n",
    "metric_values = list()\n",
    "writer = SummaryWriter()  \n",
    "lr_set = []\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print(f\"Epochs {epoch + 1}/{total_epochs}\")\n",
    "\n",
    "    model.train()  \n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for i, batch_data in enumerate(train_loader):\n",
    "\n",
    "        step += 1\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size   #epoch數\n",
    "        print(f\"Step = {step}/{epoch_len}\", end='\\r')\n",
    "\n",
    "        inputs, labels = batch_data[\"img\"].to(device), batch_data[\"seg\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs, _ = model(inputs) #forward\n",
    "        \n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += float(loss.item())\n",
    "\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    train_loss.append(epoch_loss)\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_images = None\n",
    "            val_labels = None\n",
    "            val_outputs = None\n",
    "            steps = 0\n",
    "            loss_val = 0\n",
    "\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = val_data[\"img\"].to(device), val_data[\"seg\"].to(device)\n",
    "                \n",
    "                val_outputs, _ = model(val_images) #forward\n",
    "                val_outputs = Resize([-1, 1716, 942])(val_outputs)\n",
    "\n",
    "                val_loss = monai.losses.DiceLoss(sigmoid=True)(val_outputs, val_labels)\n",
    "                loss_val += float(val_loss)\n",
    "\n",
    "                # val \n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]                \n",
    "\n",
    "                # compute metric for current iteration\n",
    "                if  steps < 5:\n",
    "                    print(\"validation loss = \", val_loss)\n",
    "                    visualize( \n",
    "                        image=val_images[0].cpu().permute(1,2,0), \n",
    "                        ground_truth_mask=val_labels[0].cpu().permute(1,2,0), \n",
    "                        predicted_mask=val_outputs[0].cpu().permute(1,2,0)\n",
    "\n",
    "                    )  \n",
    "                steps += 1\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "\n",
    "            # aggregate the final mean dice result\n",
    "            print(\"val_loss = \", loss_val / steps)\n",
    "            val_loss_ave = loss_val / steps\n",
    "\n",
    "            # reset the status for next validation round\n",
    "            dice_metric.reset()\n",
    "            metric_values.append(val_loss_ave)\n",
    "            test_loss.append(val_loss_ave)\n",
    "\n",
    "            if val_loss_ave < best_metric:\n",
    "                best_metric = val_loss_ave\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), f\"{encode_mod}.pth\")\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                \"current epoch: {} current val mean dice loss: {:.4f} best val mean dice loss: {:.4f} at epoch {}\".format(\n",
    "                    epoch + 1, val_loss_ave, best_metric, best_metric_epoch\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            torch.save(model.state_dict(), f\"last_epoch_{encode_mod}.pth\")\n",
    "        \n",
    "     #  lr decrease\n",
    "#     lr_set.append(schedular.get_last_lr()[0])\n",
    "#     schedular.step()\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"EfficientV2.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dice_metric.reset()\n",
    "with torch.no_grad():\n",
    "    val_images = None\n",
    "    val_labels = None\n",
    "    val_outputs = None\n",
    "#     show_val = True\n",
    "    for val_data in val_loader:\n",
    "        val_images, val_labels = val_data[\"img\"].to(device), val_data[\"seg\"].to(device)\n",
    "\n",
    "        val_outputs, _ = model(val_images) #forward\n",
    "        val_outputs = Resize([-1, 1716, 942])(val_outputs)\n",
    "        val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]                \n",
    "        \n",
    "        visualize( \n",
    "            image=val_images[0].cpu().permute(1,2,0), \n",
    "            ground_truth_mask=val_labels[0].cpu().permute(1,2,0), \n",
    "            predicted_mask=val_outputs[0].cpu().permute(1,2,0)\n",
    "        ) \n",
    "        \n",
    "        # compute metric for current iteration\n",
    "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "        print(dice_metric.aggregate())\n",
    "    # aggregate the final mean dice result\n",
    "    metric = dice_metric.aggregate().item()\n",
    "    print(\"metric = \", metric)\n",
    "    # reset the status for next validation round\n",
    "    dice_metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Public Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = \"./AICUP/Demo_Image/Public_Image/\"\n",
    "test_images = sorted(glob.glob(os.path.join(tempdir, \"*.jpg\")))\n",
    "\n",
    "\n",
    "print(f\" {len(test_images)} test_images\")\n",
    "\n",
    "test_files = [{\"img\": img} for img in test_images[:]]\n",
    "test_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\"]),\n",
    "        AsChannelFirstd(keys=[\"img\"]),\n",
    "        ScaleIntensityd(keys=[\"img\"]),\n",
    "        Resized(keys=[\"img\"], spatial_size=[800, 800]),\n",
    "        EnsureTyped(keys=[\"img\"])\n",
    "    ]\n",
    ")\n",
    "test_ds = monai.data.Dataset(data=test_files, transform=test_transforms)\n",
    "test_loader = DataLoader(test_ds, batch_size=1,  collate_fn=list_data_collate)\n",
    "alls = sorted(glob.glob(tempdir + \"*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, test_data in enumerate(test_loader):\n",
    "        test_images = test_data[\"img\"].to(device)\n",
    "\n",
    "        test_outputs, _ = model(test_images) #forward\n",
    "        test_outputs = Resize([-1, 1716, 942])(test_outputs)\n",
    "        \n",
    "        saverPD = SaveImage(output_dir=f\"./{encode_mod}/outputs1\", output_ext=\".png\", output_postfix=f\"{alls[i].split('/')[-1].split('.')[0]}\",scale=255,separate_folder=False)\n",
    "        saverPD(test_outputs[0].cpu())\n",
    "        \n",
    "        test_outputs = [post_trans(i) for i in decollate_batch(test_outputs)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = sorted(glob.glob(f\"./{encode_mod}/outputs1/*.png\"))\n",
    "alls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jss in alls:\n",
    "    os.rename(jss, os.path.join(*jss.split(\"/\")[:-1], jss.split(\"/\")[-1].split(\"_\", 1)[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Private Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = \"./AICUP/Demo_Image/Private_Image/\"\n",
    "private_images = sorted(glob.glob(os.path.join(tempdir, \"*.jpg\")))\n",
    "\n",
    "\n",
    "print(f\" {len(private_images)} private_images\")\n",
    "\n",
    "private_files = [{\"img\": img} for img in private_images[:]]\n",
    "private_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\"]),\n",
    "        AsChannelFirstd(keys=[\"img\"]),\n",
    "        ScaleIntensityd(keys=[\"img\"]),\n",
    "        Resized(keys=[\"img\"], spatial_size=[800, 800]),\n",
    "        EnsureTyped(keys=[\"img\"])\n",
    "    ]\n",
    ")\n",
    "test_ds = monai.data.Dataset(data= private_files, transform=test_transforms)\n",
    "test_loader = DataLoader(test_ds, batch_size=1,  collate_fn=list_data_collate)\n",
    "alls = sorted(glob.glob(tempdir + \"*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "steppp = 0\n",
    "with torch.no_grad():\n",
    "    for i, test_data in enumerate(test_loader):\n",
    "        test_images = test_data[\"img\"].to(device)\n",
    "        test_outputs, _ = model(test_images) #forward\n",
    "        test_outputs = Resize([-1, 1716, 942])(test_outputs)\n",
    "\n",
    "        saverPD = SaveImage(output_dir=f\"./{encode_mod}/outputsp\", output_ext=\".png\", output_postfix=f\"{alls[i].split('/')[-1].split('.')[0]}\",scale=255,separate_folder=False)\n",
    "        saverPD(test_outputs[0].cpu())\n",
    "        \n",
    "        test_outputs = [post_trans(i) for i in decollate_batch(test_outputs)]        \n",
    "        imgg = Image.open(alls[i])\n",
    "    \n",
    "        if steppp%30 == 0:\n",
    "            visualize( \n",
    "                image=test_images[0].cpu().permute(1,2,0), \n",
    "                ground_truth_images=imgg, \n",
    "                predicted_mask=test_outputs[0].squeeze().cpu().numpy().round()\n",
    "            ) \n",
    "        steppp += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alls = sorted(glob.glob(f\"./{encode_mod}/outputsp/*.png\"))\n",
    "alls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jss in alls:\n",
    "    os.rename(jss, os.path.join(*jss.split(\"/\")[:-1], jss.split(\"/\")[-1].split(\"_\", 1)[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv EfficientV2/outputsp/* EfficientV2/outputs1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r EfficientV2.zip EfficientV2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
